{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477c6d2e",
   "metadata": {},
   "source": [
    "# Sgkit: benchmarking Numba vs JAX\n",
    "\n",
    "We have used Numba to accelerate CPU computations in sgkit for a long time. This notebook is an experiment to compare the performance of some basic allele counting code that uses Numba with the equivalent in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c040f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sgkit as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0650a",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "We'll start by running the Numba function `count_call_alleles` from sgkit, over a variety of different sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9365d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgkit import count_call_alleles as count_call_alleles_numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19d0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [(10000, 100), (100000, 1000), (20000, 5000), (10000, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1b2ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_variant: 10000, n_sample: 100, time: 0.0037958621978759766\n",
      "n_variant: 100000, n_sample: 1000, time: 0.1375119686126709\n",
      "n_variant: 20000, n_sample: 5000, time: 0.20614981651306152\n",
      "n_variant: 10000, n_sample: 10000, time: 0.22558307647705078\n"
     ]
    }
   ],
   "source": [
    "for n_variant, n_sample in matrix:\n",
    "    ds = sg.simulate_genotype_call_dataset(\n",
    "        n_variant=n_variant, n_sample=n_sample, missing_pct=0.01\n",
    "    )\n",
    "    ds = ds.chunk({\"variants\": 10000, \"samples\": 1000})\n",
    "\n",
    "    ds = count_call_alleles_numba(ds)\n",
    "    start = time.time()\n",
    "    ds = ds.load()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"n_variant: {n_variant}, n_sample: {n_sample}, time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b72f0",
   "metadata": {},
   "source": [
    "## JAX\n",
    "\n",
    "For JAX we have to implement the equivalent of `count_call_alleles`. JAX provides its own version of the NumPy API, so we can use `bincount` to implement the inner function that operates on a single dimension of the `call_genotype` array. Note that this is different to Numba where we can write loops that operate directly on arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ff3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa22b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_alleles_jax(g):\n",
    "    # jax bincount will clip to 0, so we add 2 (then truncate the array)\n",
    "    # so that we drop counts for -1 and -2 (missing or non-allele)\n",
    "    n_alleles = 2  # we have hardcoded this for the moment, see https://jax.readthedocs.io/en/latest/jit-compilation.html#marking-arguments-as-static \n",
    "    counts = jnp.bincount(g + 2, length=n_alleles + 2)\n",
    "    counts = counts[2:]\n",
    "    return jnp.astype(counts, jnp.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececfa1a",
   "metadata": {},
   "source": [
    "The user-level function is very similar to the Numba version. The main difference is that we use JAX's `vmap` and `jit` functions to vectorize and compile the `count_alleles_jax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5517191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Hashable\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from typing_extensions import Literal\n",
    "from xarray import Dataset\n",
    "\n",
    "from sgkit import variables\n",
    "from sgkit.utils import conditional_merge_datasets, create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_call_alleles_jax(\n",
    "    ds: Dataset,\n",
    "    *,\n",
    "    call_genotype: Hashable = variables.call_genotype,\n",
    "    merge: bool = True,\n",
    ") -> Dataset:\n",
    "    variables.validate(ds, {call_genotype: variables.call_genotype_spec})\n",
    "    n_alleles = ds.sizes[\"alleles\"]\n",
    "    G = da.asarray(ds[call_genotype])\n",
    "    if G.numblocks[2] > 1:\n",
    "        raise ValueError(\n",
    "            f\"Variable {call_genotype} must have only a single chunk in the ploidy dimension. \"\n",
    "            \"Consider rechunking to change the size of chunks.\"\n",
    "        )\n",
    "    shape = (G.chunks[0], G.chunks[1], n_alleles)\n",
    "\n",
    "    # call vmap twice to vectorize over first two dimensions (variants, samples)\n",
    "    count_alleles_vectorized = jax.vmap(jax.vmap(count_alleles_jax))\n",
    "\n",
    "    # jit compile\n",
    "    count_alleles_vectorized_jit = jax.jit(count_alleles_vectorized)\n",
    "\n",
    "    # precompile...\n",
    "    count_alleles_vectorized_jit(np.ones((4, 4, 2), dtype=np.int8)).block_until_ready()\n",
    "\n",
    "    new_ds = create_dataset(\n",
    "        {\n",
    "            variables.call_allele_count: (\n",
    "                (\"variants\", \"samples\", \"alleles\"),\n",
    "                da.map_blocks(\n",
    "                    count_alleles_vectorized_jit,\n",
    "                    G,\n",
    "                    chunks=shape,\n",
    "                    dtype=np.uint8,\n",
    "                    drop_axis=2,\n",
    "                    new_axis=2,\n",
    "                ),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return conditional_merge_datasets(ds, new_ds, merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6deaa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_variant: 10000, n_sample: 100, time: 0.028610944747924805\n",
      "n_variant: 100000, n_sample: 1000, time: 3.8228919506073\n",
      "n_variant: 20000, n_sample: 5000, time: 3.199921131134033\n",
      "n_variant: 10000, n_sample: 10000, time: 3.4262049198150635\n"
     ]
    }
   ],
   "source": [
    "for n_variant, n_sample in matrix:\n",
    "    ds = sg.simulate_genotype_call_dataset(\n",
    "        n_variant=n_variant, n_sample=n_sample, missing_pct=0.01\n",
    "    )\n",
    "    ds = ds.chunk({\"variants\": 10000, \"samples\": 1000})\n",
    "\n",
    "    ds = count_call_alleles_jax(ds)\n",
    "    start = time.time()\n",
    "    ds = ds.load()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"n_variant: {n_variant}, n_sample: {n_sample}, time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285d789",
   "metadata": {},
   "source": [
    "The JAX version is a lot slower than Numba - over an order of magnitude slower for the last three results.\n",
    "\n",
    "I also tried running the `bincount` code using regular NumPy and it was around 100 times slower than JAX. This tells us that both Numba and JAX both provide massive performance improvements compared to NumPy.\n",
    "\n",
    "But it's not clear if there is something wrong with the JAX code or whether it can't do as well as Numba for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c04502",
   "metadata": {},
   "source": [
    "## LAX\n",
    "\n",
    "JAX has some lower-level primitives in the LAX module that might be suitable for this problem. In particular, `jax.lax.scan` can be used for implementing a `count_alleles` function without using NumPy operations. Would this be more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990c8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "def _count_alleles(res, el):\n",
    "    res = res.at[el].add(1)\n",
    "    return res, None\n",
    "\n",
    "def count_alleles_lax(g, out):\n",
    "    counts, _ = lax.scan(_count_alleles, out, g)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785de6bf",
   "metadata": {},
   "source": [
    "Note that we pass in the output array like Numba does, rather than allocating it in the loop (like `jax.numpy` does above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23696f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_call_alleles_lax(\n",
    "    ds: Dataset,\n",
    "    *,\n",
    "    call_genotype: Hashable = variables.call_genotype,\n",
    "    merge: bool = True,\n",
    ") -> Dataset:\n",
    "    variables.validate(ds, {call_genotype: variables.call_genotype_spec})\n",
    "    n_alleles = ds.sizes[\"alleles\"]\n",
    "    G = da.asarray(ds[call_genotype])\n",
    "    if G.numblocks[2] > 1:\n",
    "        raise ValueError(\n",
    "            f\"Variable {call_genotype} must have only a single chunk in the ploidy dimension. \"\n",
    "            \"Consider rechunking to change the size of chunks.\"\n",
    "        )\n",
    "    shape = (G.chunks[0], G.chunks[1], n_alleles)\n",
    "\n",
    "    # call vmap twice to vectorize over first two dimensions (variants, samples)\n",
    "    count_alleles_vectorized = jax.vmap(jax.vmap(count_alleles_lax))\n",
    "\n",
    "    # jit compile\n",
    "    count_alleles_vectorized_jit = jax.jit(count_alleles_vectorized)\n",
    "\n",
    "    # precompile...\n",
    "    count_alleles_vectorized_jit(np.ones((4, 4, 2), dtype=np.int8), np.zeros((4, 4, 2), dtype=np.int8)).block_until_ready()\n",
    "\n",
    "    N = np.empty((G.chunks[0][0], G.chunks[1][0], n_alleles), dtype=np.uint8)\n",
    "    new_ds = create_dataset(\n",
    "        {\n",
    "            variables.call_allele_count: (\n",
    "                (\"variants\", \"samples\", \"alleles\"),\n",
    "                da.map_blocks(\n",
    "                    count_alleles_vectorized_jit,\n",
    "                    G,\n",
    "                    N,\n",
    "                    chunks=shape,\n",
    "                    dtype=np.uint8,\n",
    "                    drop_axis=2,\n",
    "                    new_axis=2,\n",
    "                ),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return conditional_merge_datasets(ds, new_ds, merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8326ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_variant: 10000, n_sample: 100, time: 0.02508997917175293\n",
      "n_variant: 100000, n_sample: 1000, time: 2.639747142791748\n",
      "n_variant: 20000, n_sample: 5000, time: 2.751932144165039\n",
      "n_variant: 10000, n_sample: 10000, time: 2.7614219188690186\n"
     ]
    }
   ],
   "source": [
    "for n_variant, n_sample in matrix:\n",
    "    ds = sg.simulate_genotype_call_dataset(\n",
    "        n_variant=n_variant, n_sample=n_sample, missing_pct=0.0\n",
    "    )\n",
    "    ds = ds.chunk({\"variants\": 10000, \"samples\": 1000})\n",
    "\n",
    "    ds = count_call_alleles_lax(ds)\n",
    "    start = time.time()\n",
    "    ds = ds.load()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"n_variant: {n_variant}, n_sample: {n_sample}, time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23768bcb",
   "metadata": {},
   "source": [
    "The LAX version is a bit faster than the JAX NumPy version - but not much, and is still around an order of magnitude slower than Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d008de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/miniforge3/envs/sgkit-local-benchmark-jax/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohappyeyeballs==2.4.0\r\n",
      "aiohttp==3.10.5\r\n",
      "aiosignal==1.3.1\r\n",
      "anyio==4.6.0\r\n",
      "appnope==0.1.4\r\n",
      "argon2-cffi==23.1.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "arrow==1.3.0\r\n",
      "asciitree==0.3.3\r\n",
      "asttokens==2.4.1\r\n",
      "asv==0.6.4\r\n",
      "asv_runner==0.2.1\r\n",
      "async-lru==2.0.4\r\n",
      "attrs==24.2.0\r\n",
      "babel==2.16.0\r\n",
      "beautifulsoup4==4.12.3\r\n",
      "bed-reader==1.0.5\r\n",
      "bleach==6.1.0\r\n",
      "bokeh==3.5.2\r\n",
      "build==1.2.2\r\n",
      "callee==0.3.1\r\n",
      "certifi==2024.8.30\r\n",
      "cffi==1.17.1\r\n",
      "cfgv==3.4.0\r\n",
      "charset-normalizer==3.3.2\r\n",
      "click==8.1.7\r\n",
      "cloudpickle==3.0.0\r\n",
      "coloredlogs==15.0.1\r\n",
      "comm==0.2.2\r\n",
      "contourpy==1.3.0\r\n",
      "coverage==7.6.1\r\n",
      "cycler==0.12.1\r\n",
      "cyvcf2==0.31.1\r\n",
      "dask==2024.8.0\r\n",
      "dask-expr==1.1.10\r\n",
      "dask-glm==0.3.2\r\n",
      "dask-ml==2024.4.4\r\n",
      "debugpy==1.8.5\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "distlib==0.3.8\r\n",
      "distributed==2024.8.0\r\n",
      "executing==2.1.0\r\n",
      "fasteners==0.19\r\n",
      "fastjsonschema==2.20.0\r\n",
      "filelock==3.16.1\r\n",
      "fonttools==4.53.1\r\n",
      "fqdn==1.5.1\r\n",
      "frozenlist==1.4.1\r\n",
      "fsspec==2024.9.0\r\n",
      "graphviz==0.20.3\r\n",
      "h11==0.14.0\r\n",
      "httpcore==1.0.5\r\n",
      "httpx==0.27.2\r\n",
      "humanfriendly==10.0\r\n",
      "hypothesis==6.112.1\r\n",
      "identify==2.6.1\r\n",
      "idna==3.10\r\n",
      "importlib_metadata==8.5.0\r\n",
      "iniconfig==2.0.0\r\n",
      "ipykernel==6.29.5\r\n",
      "ipython==8.27.0\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==8.1.5\r\n",
      "isoduration==20.11.0\r\n",
      "jax==0.4.33\r\n",
      "jaxlib==0.4.33\r\n",
      "jedi==0.19.1\r\n",
      "Jinja2==3.1.4\r\n",
      "joblib==1.4.2\r\n",
      "json5==0.9.25\r\n",
      "jsonpointer==3.0.0\r\n",
      "jsonschema==4.23.0\r\n",
      "jsonschema-specifications==2023.12.1\r\n",
      "jupyter==1.1.1\r\n",
      "jupyter-console==6.6.3\r\n",
      "jupyter-events==0.10.0\r\n",
      "jupyter-lsp==2.2.5\r\n",
      "jupyter_client==8.6.3\r\n",
      "jupyter_core==5.7.2\r\n",
      "jupyter_server==2.14.2\r\n",
      "jupyter_server_terminals==0.5.3\r\n",
      "jupyterlab==4.2.5\r\n",
      "jupyterlab_pygments==0.3.0\r\n",
      "jupyterlab_server==2.27.3\r\n",
      "jupyterlab_widgets==3.0.13\r\n",
      "kiwisolver==1.4.7\r\n",
      "llvmlite==0.43.0\r\n",
      "locket==1.0.0\r\n",
      "MarkupSafe==2.1.5\r\n",
      "matplotlib==3.9.2\r\n",
      "matplotlib-inline==0.1.7\r\n",
      "mistune==3.0.2\r\n",
      "ml_dtypes==0.5.0\r\n",
      "msgpack==1.1.0\r\n",
      "multidict==6.1.0\r\n",
      "multipledispatch==1.0.0\r\n",
      "mypy-extensions==1.0.0\r\n",
      "nbclient==0.10.0\r\n",
      "nbconvert==7.16.4\r\n",
      "nbformat==5.10.4\r\n",
      "nest-asyncio==1.6.0\r\n",
      "networkx==3.3\r\n",
      "nodeenv==1.9.1\r\n",
      "notebook==6.1.5\r\n",
      "notebook_shim==0.2.4\r\n",
      "numba==0.60.0\r\n",
      "numcodecs==0.13.0\r\n",
      "numpy==1.26.4\r\n",
      "opt-einsum==3.3.0\r\n",
      "overrides==7.7.0\r\n",
      "packaging==24.1\r\n",
      "pandas==2.2.2\r\n",
      "pandocfilters==1.5.1\r\n",
      "parso==0.8.4\r\n",
      "partd==1.4.2\r\n",
      "patsy==0.5.6\r\n",
      "pexpect==4.9.0\r\n",
      "pillow==10.4.0\r\n",
      "platformdirs==4.3.6\r\n",
      "pluggy==1.5.0\r\n",
      "pre-commit==3.8.0\r\n",
      "prometheus_client==0.21.0\r\n",
      "prompt_toolkit==3.0.47\r\n",
      "psutil==6.0.0\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure_eval==0.2.3\r\n",
      "pyarrow==17.0.0\r\n",
      "pycparser==2.22\r\n",
      "Pygments==2.18.0\r\n",
      "Pympler==1.1\r\n",
      "pyparsing==3.1.4\r\n",
      "pyproject_hooks==1.1.0\r\n",
      "pytest==8.3.3\r\n",
      "pytest-cov==5.0.0\r\n",
      "pytest-datadir==1.5.0\r\n",
      "pytest-mock==3.14.0\r\n",
      "python-dateutil==2.9.0.post0\r\n",
      "python-json-logger==2.0.7\r\n",
      "pytz==2024.2\r\n",
      "PyYAML==6.0.2\r\n",
      "pyzmq==26.2.0\r\n",
      "rechunker==0.5.2\r\n",
      "referencing==0.35.1\r\n",
      "requests==2.32.3\r\n",
      "rfc3339-validator==0.1.4\r\n",
      "rfc3986-validator==0.1.1\r\n",
      "rpds-py==0.20.0\r\n",
      "scikit-allel==1.3.13\r\n",
      "scikit-learn==1.5.2\r\n",
      "scipy==1.14.1\r\n",
      "Send2Trash==1.8.3\r\n",
      "six==1.16.0\r\n",
      "sniffio==1.3.1\r\n",
      "sortedcontainers==2.4.0\r\n",
      "soupsieve==2.6\r\n",
      "sparse==0.15.4\r\n",
      "stack-data==0.6.3\r\n",
      "statsmodels==0.14.3\r\n",
      "tabulate==0.9.0\r\n",
      "tblib==3.0.0\r\n",
      "terminado==0.18.1\r\n",
      "threadpoolctl==3.5.0\r\n",
      "tinycss2==1.3.0\r\n",
      "toolz==0.12.1\r\n",
      "tornado==6.4.1\r\n",
      "traitlets==5.14.3\r\n",
      "types-python-dateutil==2.9.0.20240906\r\n",
      "typing_extensions==4.12.2\r\n",
      "tzdata==2024.1\r\n",
      "uri-template==1.3.0\r\n",
      "urllib3==2.2.3\r\n",
      "virtualenv==20.26.5\r\n",
      "wcwidth==0.2.13\r\n",
      "webcolors==24.8.0\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.8.0\r\n",
      "widgetsnbextension==4.0.13\r\n",
      "xarray==2024.9.0\r\n",
      "xyzservices==2024.9.0\r\n",
      "yarl==1.11.1\r\n",
      "zarr==2.18.3\r\n",
      "zict==3.0.0\r\n",
      "zipp==3.20.2\r\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c899d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
